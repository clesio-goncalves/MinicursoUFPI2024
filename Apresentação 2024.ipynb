{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00ee9d5d-aca4-4ca6-b4b0-a2ff17a8b701",
      "metadata": {
        "id": "00ee9d5d-aca4-4ca6-b4b0-a2ff17a8b701"
      },
      "source": [
        "# Minicurso - Introdução à Visão Computacional e Aprendizado Profundo com Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd8432d-3e61-4a1f-a7a1-86eb7dc95efc",
      "metadata": {
        "id": "3fd8432d-3e61-4a1f-a7a1-86eb7dc95efc"
      },
      "source": [
        "# Sumário\n",
        "\n",
        "- Introdução\n",
        "- Visão Computacional\n",
        "- Aprendizado Profundo\n",
        "- Estudo de Caso\n",
        "- Avaliação dos Resultados\n",
        "- Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso da **GPU**"
      ],
      "metadata": {
        "id": "4Q97rUPNFrb9"
      },
      "id": "4Q97rUPNFrb9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso de GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejQeFULeFvuA",
        "outputId": "929df40a-5c37-46fa-b1ed-e6aace690b77"
      },
      "id": "ejQeFULeFvuA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  5 16:10:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #aqui tem q escolher uma das gpus, veja a que esta desocupada\n",
        "tf_device='/gpu:0'"
      ],
      "metadata": {
        "id": "wdm04tzzF3p-"
      },
      "id": "wdm04tzzF3p-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "adc29538-cc19-4f27-a58c-f5a8153034fb",
      "metadata": {
        "id": "adc29538-cc19-4f27-a58c-f5a8153034fb"
      },
      "source": [
        "Dataset disponível em: https://www.kaggle.com/datasets/dipuiucse/monkeypoxskinimagedataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135354e7-95b0-480e-bbd7-fe39763e7a1e",
      "metadata": {
        "id": "135354e7-95b0-480e-bbd7-fe39763e7a1e"
      },
      "source": [
        "# Estudo de Caso"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751e3f58-4991-447e-9cd7-675c4a3566bf",
      "metadata": {
        "id": "751e3f58-4991-447e-9cd7-675c4a3566bf"
      },
      "source": [
        "Estudo de caso que consiste em 4 etapas:\n",
        "\n",
        "1) Aquisição de um conjunto de imagens de varíola dos macacos, sarampo, catapora e casos normais;\n",
        "2) Utilização de técnicas de pré-processamento de imagens;\n",
        "3) Aplicação da técnica de aumento de dados aleatório para aumentar a generalização do modelo de classificação;\n",
        "4) Treinamento de modelos da CNN para extração de atributos e classificação das imagens.\n",
        "\n",
        "![title](imagens/classificacao.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d978c7fb-5491-4103-bea9-100c418237b0",
      "metadata": {
        "id": "d978c7fb-5491-4103-bea9-100c418237b0"
      },
      "source": [
        "# Aquisição de Imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d887ff48-750b-4250-9f95-c831ee2e6ceb",
      "metadata": {
        "id": "d887ff48-750b-4250-9f95-c831ee2e6ceb"
      },
      "source": [
        "Para as tarefas envolvendo o aprendizado profundo em qualquer contexto é necessária a\n",
        "aquisição de imagens que consigam representar as classes de problemas há serem estu-\n",
        "dados. Com o intuito de trazer um estudo de caso, utilizamos um conjunto de imagens\n",
        "baseado em imagens de pele disponível publicamente [Bala 2022].\n",
        "\n",
        "O conjunto de dados é dividido em quatro classes: varíola dos macacos, catapora,\n",
        "sarampo e saudáveis, contendo 107, 91, 279 e 293 imagens, respectivamente. Esse con-\n",
        "junto de dados é utilizado apenas para fins educacionais e de pesquisas.\n",
        "\n",
        "![title](imagens/classes.png)\n",
        "\n",
        "- a) Varíola dos macacos\n",
        "- b) Catapora\n",
        "- c) Sarampo\n",
        "- d) Outros (saudáveis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46183db5-d61b-4497-9b45-fcdd38646351",
      "metadata": {
        "id": "46183db5-d61b-4497-9b45-fcdd38646351"
      },
      "source": [
        "# Aumento de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6060e7-357c-4e01-8470-14c2d74db38a",
      "metadata": {
        "id": "1d6060e7-357c-4e01-8470-14c2d74db38a"
      },
      "source": [
        "Nesse processo de classificação, utiliza-se quatro técnicas de aumento de dados:\n",
        "\n",
        "1) espelhamento aleatório da imagem, nos eixos horizontal e vertical;\n",
        "2) rotação aleatória da imagem variando de 0° a 90° graus, no sentido horário e anti-horário, esse intervalo foi aplicado para preservar as características da imagem;\n",
        "3) zoom aleatório variando de 0% a 10%, pois valores mais altos podem causar a perda de partes importantes da imagem, como as amastigotas;\n",
        "4) contraste da imagem ajustado por um valor de fator de 0,2, o fator de contraste é escolhido aleatoriamente entre [1,0 - fator, 1,0 + fator]. O contraste é ajustado de forma independente para cada canal de cor de cada imagem durante o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2f452e-1eab-435e-9dce-170c7dfb3296",
      "metadata": {
        "id": "ad2f452e-1eab-435e-9dce-170c7dfb3296"
      },
      "source": [
        "# Extração de Atributos e Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ead0b75-f310-4fa4-b31a-6dbda61f6044",
      "metadata": {
        "id": "9ead0b75-f310-4fa4-b31a-6dbda61f6044"
      },
      "source": [
        "![title](imagens/tabelamodel.png)\n",
        "\n",
        "![title](imagens/parametros.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b2dc445-d378-478c-bfee-bc1de0ca3adb",
      "metadata": {
        "id": "5b2dc445-d378-478c-bfee-bc1de0ca3adb"
      },
      "source": [
        "Para avaliar a metodologia completa e comprovar a generalização dos modelos\n",
        "na base de teste são utilizadas algumas métricas que são amplamente difundidas na liter-\n",
        "atura, temos então as seguintes métricas: Acurácia; Kappa; Recall; Precisão; F1-Score;\n",
        "Especificidade e AUC.\n",
        "\n",
        "Acurácia [Story and Congalton 1986](Acc): é a média global dos acertos para o\n",
        "modelo ao classificar as classes. A Acurácia se da pela Equação 1.\n",
        "\n",
        "![title](imagens/acc.png)\n",
        "\n",
        "Kappa [Cohen 1968]: a acurácia pode não ser uma boa métrica de avaliação para\n",
        "dados que possam conter classes com quantidade de amostras desbalanceados, então para\n",
        "resolver isso podemos utilizar a métrica Kappa, dada pela Equação 2.\n",
        "\n",
        "![title](imagens/kappa.png)\n",
        "\n",
        "Recall (Sensibilidade): taxa real positiva (TPR), taxa de acerto ou sensibilidade,\n",
        "de um classificador representa as amostras classificadas corretamente para o número total\n",
        "de amostras positivas [Tharwat 2021]. Ela pode ser obtida pela equação 3.\n",
        "\n",
        "![title](imagens/recall.png)\n",
        "\n",
        "Precisão [David L. Olson 2008]: essa métrica verifica quantas observações o mod-\n",
        "elo classificou de forma correta como pertencente a uma classe. A Precisão se da pela\n",
        "Equação 4.\n",
        "\n",
        "![title](imagens/prec.png)\n",
        "\n",
        "F1-Score [Rijsbergen 1979](F1): a métrica F1-Score é a média harmônica entre\n",
        "as métricas, Recall e Precisão, ou seja, ela resume as informações dessas duas métricas.\n",
        "A formula dessa métrica é dada pela Equação 5.\n",
        "\n",
        "![title](imagens/f1.png)\n",
        "\n",
        "Especificidade (ESP): é expressa como a razão das amostras negativas correta-\n",
        "mente classificadas para o número total de amostras negativas. Assim, a especificidade\n",
        "representa a proporção das amostras negativas corretamente classificadas [Tharwat 2021].\n",
        "Podemos calculá-la usando a Equação 6.\n",
        "\n",
        "![title](imagens/esp.png)\n",
        "\n",
        "A curva ROC (Receiver Operating Characteristics) é um gráfico de linha que\n",
        "mostra a capacidade de diagnóstico de um classificador, visualizando seu desempenho\n",
        "com diferentes limiares. A área sob a curva ROC, do inglês Area Under the Curve\n",
        "(AUC), mede toda a área bidimensional abaixo da curva ROC. Essa curva representa\n",
        "dois parâmetros: Taxa de Verdadeiro Positivo (TPR) e Taxa de Falso Positivo (FPR)\n",
        "[Taha and Hanbury 2015].\n",
        "\n",
        "![title](imagens/roc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "actual-broadcasting",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "actual-broadcasting",
        "outputId": "5573caea-4ae4-4c46-d584-6d39c3331a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# Versão da Linguagem Python\n",
        "from platform import python_version\n",
        "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso da memória RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Esse notebook tem {:.1f} gigabytes de RAM disponível\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Não está executando com muita RAM')\n",
        "else:\n",
        "  print('Você está usando um ambiente de execução com muita RAM!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6vYLzNAF-YN",
        "outputId": "77b7bf78-0a67-46f5-eb0c-d2eb71a57ddc"
      },
      "id": "H6vYLzNAF-YN",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esse notebook tem 89.6 gigabytes de RAM disponível\n",
            "\n",
            "Você está usando um ambiente de execução com muita RAM!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A4i0yLPGD_q",
        "outputId": "3617092f-3ca5-4895-d94e-bfd2a22f4879"
      },
      "id": "0A4i0yLPGD_q",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp -R /content/gdrive/MyDrive/Dataset ./"
      ],
      "metadata": {
        "id": "2RQ7X3wuGPsA"
      },
      "id": "2RQ7X3wuGPsA",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir modelo"
      ],
      "metadata": {
        "id": "eDDphSseH4I8"
      },
      "id": "eDDphSseH4I8",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jHtW-liGaZq",
        "outputId": "e415bdce-9050-4e32-9ff7-ddfeb76a619f"
      },
      "id": "7jHtW-liGaZq",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting watermark\n",
            "  Downloading watermark-2.5.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.34.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from watermark) (8.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from watermark) (75.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->watermark) (3.20.2)\n",
            "Collecting jedi>=0.16 (from ipython>=6.0->watermark)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->watermark) (0.2.13)\n",
            "Downloading watermark-2.5.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, watermark\n",
            "Successfully installed jedi-0.19.1 watermark-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "related-technique",
      "metadata": {
        "id": "related-technique"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "diverse-charger",
      "metadata": {
        "id": "diverse-charger"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "# Imports para manipulação e visualização de dados\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imports para manipulação de imagens\n",
        "import os\n",
        "import sklearn\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Imports para cálculo de métricas e utilitários\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Imports para Deep Learning\n",
        "#import plaidml\n",
        "#import plaidml.keras\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
        "\n",
        "# Redes Neurais\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.applications.resnet_v2 import ResNet152V2\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# As novas versões do Pandas e Matplotlib trazem diversas mensagens de aviso ao desenvolvedor. Vamos desativar isso.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fossil-draft",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fossil-draft",
        "outputId": "d04e30a4-92e8-47e9-84be-b4d571d6cf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: IFSertão\n",
            "\n",
            "sklearn   : 1.5.2\n",
            "tensorflow: 2.17.0\n",
            "PIL       : 10.4.0\n",
            "matplotlib: 3.8.0\n",
            "platform  : 1.0.8\n",
            "google    : 2.0.3\n",
            "pandas    : 2.2.2\n",
            "keras     : 3.4.1\n",
            "sys       : 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
            "numpy     : 1.26.4\n",
            "psutil    : 5.9.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"IFSertão\" --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chemical-coating",
      "metadata": {
        "id": "chemical-coating"
      },
      "source": [
        "## Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "atmospheric-humidity",
      "metadata": {
        "id": "atmospheric-humidity"
      },
      "outputs": [],
      "source": [
        "# Dados de entrada\n",
        "#PATH_IMAGES = 'dataset/Monkeypox_2_classes/Original_Images/Original_Images/'\n",
        "PATH_IMAGES = 'Dataset'\n",
        "\n",
        "# Parâmetros do treinamento\n",
        "BATCH_SIZE = 8\n",
        "SEED = 42\n",
        "\n",
        "# K-fold\n",
        "N_SPLIT = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "accepted-memphis",
      "metadata": {
        "id": "accepted-memphis"
      },
      "outputs": [],
      "source": [
        "# Parâmetros treinamento\n",
        "epocas_treinamento = 20\n",
        "base_learning_rate = 0.0001\n",
        "\n",
        "epocas_ajuste_fino = 10\n",
        "total_epocas = epocas_treinamento + epocas_ajuste_fino"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "different-exposure",
      "metadata": {
        "id": "different-exposure"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "joint-geneva",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joint-geneva",
        "outputId": "75bfaa4a-d753-4160-8207-bed288f26049"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#CATEGORIAS = ['Monkey Pox', 'Others']\n",
        "CATEGORIAS = ['Chickenpox', 'Measles', 'Monkeypox', 'Normal']\n",
        "NUM_CATEGORIAS = len(CATEGORIAS)\n",
        "NUM_CATEGORIAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "coral-tuition",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coral-tuition",
        "outputId": "bed233ef-2e23-49c0-e2f8-aabf9fde8b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chickenpox 107 images\n",
            "Measles 91 images\n",
            "Monkeypox 289 images\n",
            "Normal 293 images\n"
          ]
        }
      ],
      "source": [
        "for category in CATEGORIAS:\n",
        "    print('{} {} images'.format(category, len(os.listdir(os.path.join(PATH_IMAGES, category)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "armed-worry",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "armed-worry",
        "outputId": "29143c5d-eab1-4ed6-e7d4-5c009fa8c1e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(780, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dataset = []\n",
        "for category in CATEGORIAS:\n",
        "    for file in os.listdir(os.path.join(PATH_IMAGES, category)):\n",
        "        dataset.append(['{}/{}'.format(category, file), category])\n",
        "dataset = pd.DataFrame(dataset, columns=['arquivo', 'categoria'])\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "mechanical-fever",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mechanical-fever",
        "outputId": "b6b0896c-7923-4d88-fe51-e8517bb3de3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          arquivo   categoria\n",
              "0     Chickenpox/chickenpox88.png  Chickenpox\n",
              "1    Chickenpox/chickenpox102.png  Chickenpox\n",
              "2     Chickenpox/chickenpox59.png  Chickenpox\n",
              "3     Chickenpox/chickenpox53.png  Chickenpox\n",
              "4     Chickenpox/chickenpox12.png  Chickenpox\n",
              "..                            ...         ...\n",
              "775           Normal/normal51.png      Normal\n",
              "776           Normal/normal88.png      Normal\n",
              "777          Normal/normal251.png      Normal\n",
              "778           Normal/normal11.png      Normal\n",
              "779          Normal/normal261.png      Normal\n",
              "\n",
              "[780 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0926437-dfc9-42a4-a502-bad2c71c33fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>arquivo</th>\n",
              "      <th>categoria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chickenpox/chickenpox88.png</td>\n",
              "      <td>Chickenpox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chickenpox/chickenpox102.png</td>\n",
              "      <td>Chickenpox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chickenpox/chickenpox59.png</td>\n",
              "      <td>Chickenpox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chickenpox/chickenpox53.png</td>\n",
              "      <td>Chickenpox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chickenpox/chickenpox12.png</td>\n",
              "      <td>Chickenpox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>Normal/normal51.png</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>Normal/normal88.png</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>Normal/normal251.png</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>Normal/normal11.png</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>Normal/normal261.png</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>780 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0926437-dfc9-42a4-a502-bad2c71c33fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0926437-dfc9-42a4-a502-bad2c71c33fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0926437-dfc9-42a4-a502-bad2c71c33fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2456f3d6-7f16-4a60-857c-ca49be66efff\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2456f3d6-7f16-4a60-857c-ca49be66efff')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2456f3d6-7f16-4a60-857c-ca49be66efff button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0bc83269-dda7-4d07-a028-500b53db7eea\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0bc83269-dda7-4d07-a028-500b53db7eea button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 780,\n  \"fields\": [\n    {\n      \"column\": \"arquivo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 780,\n        \"samples\": [\n          \"Normal/normal181.png\",\n          \"Normal/normal254.png\",\n          \"Normal/normal262.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categoria\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Measles\",\n          \"Normal\",\n          \"Chickenpox\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "preliminary-booth",
      "metadata": {
        "id": "preliminary-booth"
      },
      "outputs": [],
      "source": [
        "# Definindo X e Y\n",
        "dataset_x = dataset.arquivo\n",
        "dataset_y = dataset.categoria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continental-request",
      "metadata": {
        "id": "continental-request"
      },
      "source": [
        "# Leitura dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "stone-harris",
      "metadata": {
        "id": "stone-harris"
      },
      "outputs": [],
      "source": [
        "#Initializing Data Generators\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# train_datagen = ImageDataGenerator(rescale = 1.0/255,\n",
        "#                                     zoom_range = 0.2,\n",
        "#                                     horizontal_flip = True,\n",
        "#                                     vertical_flip = True,\n",
        "#                                     rotation_range = 90,\n",
        "#                                     brightness_range = (-0.1, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "settled-mechanism",
      "metadata": {
        "id": "settled-mechanism"
      },
      "outputs": [],
      "source": [
        "# Carrega os dados de treino, validação e testes\n",
        "def leitura_dados(img_size):\n",
        "    train_dataset = datagen.flow_from_dataframe(dataframe = train_df,\n",
        "                                                      directory = PATH_IMAGES,\n",
        "                                                      x_col = \"arquivo\",\n",
        "                                                      y_col = \"categoria\",\n",
        "                                                      class_mode = \"categorical\",\n",
        "                                                      target_size = img_size,\n",
        "                                                      batch_size = BATCH_SIZE,\n",
        "                                                      seed = SEED,\n",
        "                                                      shuffle = True)\n",
        "\n",
        "    val_dataset = datagen.flow_from_dataframe(dataframe = val_df,\n",
        "                                                      directory = PATH_IMAGES,\n",
        "                                                      x_col = \"arquivo\",\n",
        "                                                      y_col = \"categoria\",\n",
        "                                                      class_mode = \"categorical\",\n",
        "                                                      target_size = img_size,\n",
        "                                                      batch_size = BATCH_SIZE,\n",
        "                                                      shuffle = False)\n",
        "\n",
        "    test_dataset = datagen.flow_from_dataframe(dataframe = test_df,\n",
        "                                                      directory = PATH_IMAGES,\n",
        "                                                      x_col = \"arquivo\",\n",
        "                                                      y_col = \"categoria\",\n",
        "                                                      class_mode = \"categorical\",\n",
        "                                                      target_size = img_size,\n",
        "                                                      batch_size = BATCH_SIZE,\n",
        "                                                      shuffle = False)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "offensive-victor",
      "metadata": {
        "id": "offensive-victor"
      },
      "source": [
        "# Métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "historical-simple",
      "metadata": {
        "id": "historical-simple"
      },
      "outputs": [],
      "source": [
        "# Cria data frame de resultados\n",
        "colunas_dataframe = ['model', 'kappa', 'accuracy', 'recall', 'precision', 'f1', 'specificity', 'roc']\n",
        "resultados = pd.DataFrame(columns = colunas_dataframe)\n",
        "\n",
        "def metrics_multi_class(y_true, y_pred):\n",
        "    '''\n",
        "        fonte: https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872\n",
        "    '''\n",
        "\n",
        "    global resultados\n",
        "\n",
        "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
        "\n",
        "    fp = (cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)).astype(float)\n",
        "    fn = (cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)).astype(float)\n",
        "    tp = (np.diag(cnf_matrix)).astype(float)\n",
        "    tn = (cnf_matrix.sum() - (fp + fn + tp)).astype(float)\n",
        "\n",
        "    weights = np.sum(cnf_matrix, axis=1)\n",
        "    specificity_array = (tn / (tn + fp))\n",
        "    specificity = np.sum(specificity_array * weights) / np.sum(weights)\n",
        "\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted') # sensibility\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # ROC\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_true)\n",
        "    true = lb.transform(y_true)\n",
        "    pred = lb.transform(y_pred)\n",
        "    roc = roc_auc_score(true, pred)\n",
        "\n",
        "    metricas = {'model': models[rede].name,\n",
        "                'kappa': kappa,\n",
        "                'accuracy': accuracy,\n",
        "                'recall': recall,\n",
        "                'precision': precision,\n",
        "                'f1': f1,\n",
        "                'specificity':specificity,\n",
        "                'roc': roc}\n",
        "\n",
        "    print(metricas)\n",
        "\n",
        "    metricas_df = pd.DataFrame([metricas])\n",
        "    resultados = pd.concat([resultados, metricas_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "regulated-underwear",
      "metadata": {
        "id": "regulated-underwear"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Função para a Matriz de Confusão\n",
        "def plot_confusion_matrix(normalize = False,\n",
        "                          title = 'Matriz de Confusão',\n",
        "                          cmap = plt.cm.Blues):\n",
        "\n",
        "    classes = CATEGORIAS\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Se normalize = True, obtemos a matriz de confusão com dados normalizados\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
        "        print(\"Matriz de Confusão Normalizada\")\n",
        "    else:\n",
        "        print('Matriz de Confusão Sem Normalização')\n",
        "\n",
        "    # Mostramos a Matriz de Confusão\n",
        "    #print(cm)\n",
        "\n",
        "    # Plot\n",
        "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation = 45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    # Plot do texto\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment = \"center\",\n",
        "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('Label Verdadeiro')\n",
        "    plt.xlabel('Label Previsto')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "russian-parks",
      "metadata": {
        "id": "russian-parks"
      },
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "supported-crash",
      "metadata": {
        "id": "supported-crash"
      },
      "outputs": [],
      "source": [
        "# leitura dos pesos pré-treinados da ImageNet\n",
        "# Não inclui as camadas de classificação no topo, ideal para extração de features\n",
        "models = [\n",
        "    VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False),\n",
        "    #DenseNet201(weights = \"imagenet\", input_shape = (224, 224, 3), include_top = False),\n",
        "    #InceptionV3(weights='imagenet', input_shape=(224, 224, 3), include_top=False),\n",
        "    #Xception(weights='imagenet', input_shape=(224, 224, 3), include_top=False),\n",
        "    InceptionResNetV2(weights='imagenet', input_shape=(224, 224, 3), include_top=False),\n",
        "    #NASNetLarge(weights='imagenet', input_shape=(224, 224, 3), include_top=False),\n",
        "    #ResNet152V2(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
        "]\n",
        "\n",
        "# Reescala dos valores dos pixels do modelo\n",
        "processamento_input = [\n",
        "    keras.applications.vgg16.preprocess_input,\n",
        "    #keras.applications.densenet.preprocess_input,\n",
        "    #keras.applications.inception_v3.preprocess_input,\n",
        "    #keras.applications.xception.preprocess_input,\n",
        "    keras.applications.inception_resnet_v2.preprocess_input,\n",
        "    #keras.applications.nasnet.preprocess_input,\n",
        "    #keras.applications.resnet_v2.preprocess_input\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "australian-fantasy",
      "metadata": {
        "id": "australian-fantasy"
      },
      "outputs": [],
      "source": [
        "# Construir um modelo a partir de redes neurais pré-treinadas\n",
        "def modelo_base(rede):\n",
        "\n",
        "    base_model = models[rede]\n",
        "    img_size = (base_model.input.shape[1], base_model.input.shape[2])\n",
        "\n",
        "    # Processa as entradas do modelo\n",
        "    preprocess_input = processamento_input[rede]\n",
        "\n",
        "    return base_model, img_size, preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "major-success",
      "metadata": {
        "id": "major-success"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        keras.layers.RandomRotation(0.9),\n",
        "        keras.layers.RandomZoom(0.1),\n",
        "        keras.layers.RandomContrast(0.2)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "missing-labor",
      "metadata": {
        "id": "missing-labor"
      },
      "outputs": [],
      "source": [
        "# Construir modelo\n",
        "def build_model():\n",
        "\n",
        "    # Freeze the base_model\n",
        "    # Congela a base convolucional antes de compilar e treinar o modelo\n",
        "    # Evita que os pesos em uma determinada camada sejam atualizados durante o treinamento\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Arquitetura do modelo básico\n",
        "    # base_model.summary()\n",
        "\n",
        "    # Adiciona o cabeçalho de classificação\n",
        "    # Converter as features do shape `base_model.output_shape[1:] para vectores\n",
        "    global_average_layer = keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "    # Aplica uma camada densa para converter essas features em uma única previsão por imagem\n",
        "    # Os números > 0.5 preveem a classe 1, os números <= 0.5 preveem a classe 0\n",
        "    prediction_layer = keras.layers.Dense(4, activation=\"softmax\", name='predictions') # Função de ativação sigmoid adicionada\n",
        "\n",
        "    # Modelo encadeando as camadas de aumento de dados, reescalonamento, base_model e extrator de features\n",
        "    img_shape = img_size + (3,)\n",
        "    inputs = keras.Input(shape = img_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = preprocess_input(x)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "    x = keras.layers.Dropout(0.2)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "threatened-packaging",
      "metadata": {
        "id": "threatened-packaging"
      },
      "outputs": [],
      "source": [
        "# Compile o modelo antes de treiná-lo\n",
        "def compile_model(learning_rate):\n",
        "\n",
        "    # Compilar modelo\n",
        "    # Não especifiquei o batch_size, pois os dados já estão em conjuntos (batchs)\n",
        "    model.compile(optimizer = Adam(learning_rate = learning_rate), loss = keras.losses.CategoricalCrossentropy(), metrics = ['categorical_accuracy'])\n",
        "\n",
        "    # model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "normal-maker",
      "metadata": {
        "id": "normal-maker"
      },
      "outputs": [],
      "source": [
        "# Definimos um checkpoint para verificar regularmente se a perda em validação diminuiu\n",
        "# Se a performance melhorar em validação salvamos o modelo\n",
        "# Podemos ainda optar por salvar o modelo a cada número de épocas\n",
        "# callbacks\n",
        "# Redução gradual da taxa de aprendizado (Reduce on Plateau)\n",
        "def get_callbacks():\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        ModelCheckpoint(\n",
        "            filepath='modelo/{}.weights.h5'.format(models[rede].name),  # Nome do arquivo com .weights.h5\n",
        "            verbose=1,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=True\n",
        "        )\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "interior-solomon",
      "metadata": {
        "id": "interior-solomon"
      },
      "outputs": [],
      "source": [
        "# Treinamento do modelo\n",
        "def treinamento_model(qnt_epocas, epoca_inicial):\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs = qnt_epocas,\n",
        "                        initial_epoch = epoca_inicial,\n",
        "                        validation_data = val_dataset,\n",
        "                        verbose=1,\n",
        "                        callbacks = get_callbacks())\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "worse-patch",
      "metadata": {
        "id": "worse-patch"
      },
      "outputs": [],
      "source": [
        "# Curvas de aprendizado da precisão / perda de treinamento e validação ao usar o modelo\n",
        "def aprendizado_treinamento():\n",
        "\n",
        "    metricas_graficos = [\"loss\", \"categorical_accuracy\"]\n",
        "\n",
        "    fig, ax = plt.subplots(len(metricas_graficos), 1, figsize=(10, len(metricas_graficos)*6))\n",
        "    ax = ax.ravel()\n",
        "    dados_x = np.arange(1, epocas_treinamento+1, 1)\n",
        "\n",
        "    for i, met in enumerate(metricas_graficos):\n",
        "        ax[i].plot(dados_x, history.history[met], label='Training ' + met)\n",
        "        ax[i].plot(dados_x, history.history[\"val_\" + met], label='Validation ' + met)\n",
        "        ax[i].set_title(\"Training and Validation %s in model %s\" %(met, models[rede].name))\n",
        "        ax[i].set_xlabel(\"epochs\")\n",
        "        ax[i].set_ylabel(met)\n",
        "\n",
        "        if (i != 0): # loss function\n",
        "            ax[i].legend(loc='lower right')\n",
        "        else:\n",
        "            ax[i].legend(loc='upper right')\n",
        "\n",
        "    return metricas_graficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "regular-makeup",
      "metadata": {
        "id": "regular-makeup"
      },
      "outputs": [],
      "source": [
        "# LIMIAR PARA AJUSTE FINO (OPCIONAL)\n",
        "# Definir as camadas inferiores como não treináveis\n",
        "def limiar_ajuste_fino():\n",
        "\n",
        "    # Exibe a quantidade de camadas do modelo base\n",
        "    # print(\"Número de camadas no modelo base: \", len(base_model.layers))\n",
        "\n",
        "    # Ajuste fino desta camada em diante\n",
        "    fine_tune_at = 100\n",
        "\n",
        "    # Congele todas as camadas antes da camada 'fine_tune_at'\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "environmental-stone",
      "metadata": {
        "id": "environmental-stone"
      },
      "outputs": [],
      "source": [
        "# Ajuste fino\n",
        "def ajuste_fino():\n",
        "\n",
        "    # Foi treinado apenas algumas camadas do modelo.\n",
        "    # Os pesos da rede pré-treinada não foram atualizados durante o treinamento.\n",
        "    # Uma maneira de aumentar ainda mais o desempenho é treinar (ou \"ajustar\") os pesos das camadas superiores\n",
        "    # do modelo pré-treinado junto com o treinamento do classificador adicionado (camada de classificação adicionada)\n",
        "    # Descongelar as camadas superiores do modelo (descongelar base_model)\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Limiar ajuste fino (OPCIONAL)\n",
        "    limiar_ajuste_fino()\n",
        "\n",
        "    # É necessário recompilar o modelo (para que essas alterações tenham efeito)\n",
        "    # É importante usar uma taxa de aprendizado mais baixa neste estágio,\n",
        "    # pois está usando um modelo muito maior e deseja readaptar os pesos pré-treinados\n",
        "    compile_model(base_learning_rate/10)\n",
        "\n",
        "    # Retomar o treinamento melhorará sua precisão em alguns pontos percentuais\n",
        "    # history.epoch[-1] é a última época do ultimo treinamento\n",
        "    history_fine = treinamento_model(total_epocas, history.epoch[-1]+1)\n",
        "\n",
        "    return history_fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "standing-think",
      "metadata": {
        "id": "standing-think"
      },
      "outputs": [],
      "source": [
        "# Curvas de aprendizado da precisão / perda de treinamento e validação ao ajustar as últimas camadas do modelo\n",
        "def aprendizado_ajuste_fino():\n",
        "\n",
        "    fig, ax = plt.subplots(len(metricas_graficos), 1, figsize=(12, len(metricas_graficos)*6))\n",
        "    ax = ax.ravel()\n",
        "    dados_x = np.arange(1, len(history.history['loss']) + len(history_fine.history['loss'])+1, 1)\n",
        "\n",
        "    for i, met in enumerate(metricas_graficos):\n",
        "        dados_treino = history.history[met] + history_fine.history[met]\n",
        "        dados_validacao = history.history[\"val_\" + met] + history_fine.history[\"val_\" + met]\n",
        "\n",
        "        ax[i].plot(dados_x, dados_treino, label='Training ' + met)\n",
        "        ax[i].plot(dados_x, dados_validacao, label='Validation ' + met)\n",
        "        ax[i].plot([epocas_treinamento, epocas_treinamento], plt.ylim(), label='Start Fine Tuning')\n",
        "        ax[i].set_title(\"Training and Validation %s in model %s\" %(met, models[rede].name))\n",
        "        ax[i].set_xlabel(\"epochs\")\n",
        "        ax[i].set_ylabel(met)\n",
        "\n",
        "        if (i != 0): # loss function\n",
        "            ax[i].legend(loc='lower right')\n",
        "        else:\n",
        "            ax[i].legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "palestinian-school",
      "metadata": {
        "id": "palestinian-school"
      },
      "source": [
        "# Treinamento K-fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greek-listening",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "greek-listening",
        "outputId": "e8dda0f8-90ef-4c8a-e9bf-8de864d9ff50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================\n",
            "Iteração 1 de 4\n",
            "======================================================\n",
            "\n",
            "Executando modelo vgg16\n",
            "Found 585 validated image filenames belonging to 4 classes.\n",
            "Found 78 validated image filenames belonging to 4 classes.\n",
            "Found 117 validated image filenames belonging to 4 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m 1/74\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 2s/step - categorical_accuracy: 0.2500 - loss: 8.5020"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.2484 - loss: 4.4681\n",
            "Epoch 1: val_loss improved from inf to 3.89523, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - categorical_accuracy: 0.2497 - loss: 4.4308 - val_categorical_accuracy: 0.2821 - val_loss: 3.8952 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.3612 - loss: 2.9379\n",
            "Epoch 2: val_loss improved from 3.89523 to 3.34788, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.3598 - loss: 2.9519 - val_categorical_accuracy: 0.3590 - val_loss: 3.3479 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.3469 - loss: 3.0080\n",
            "Epoch 3: val_loss improved from 3.34788 to 2.86595, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.3489 - loss: 2.9898 - val_categorical_accuracy: 0.3974 - val_loss: 2.8659 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.4494 - loss: 2.4755\n",
            "Epoch 4: val_loss improved from 2.86595 to 2.49916, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.4507 - loss: 2.4684 - val_categorical_accuracy: 0.4872 - val_loss: 2.4992 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.4893 - loss: 2.2443\n",
            "Epoch 5: val_loss improved from 2.49916 to 2.32921, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.4892 - loss: 2.2408 - val_categorical_accuracy: 0.5385 - val_loss: 2.3292 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5087 - loss: 1.9728\n",
            "Epoch 6: val_loss improved from 2.32921 to 2.17804, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.5091 - loss: 1.9723 - val_categorical_accuracy: 0.5513 - val_loss: 2.1780 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5883 - loss: 1.6349\n",
            "Epoch 7: val_loss improved from 2.17804 to 2.08051, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.5882 - loss: 1.6343 - val_categorical_accuracy: 0.5641 - val_loss: 2.0805 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - categorical_accuracy: 0.5806 - loss: 1.7737\n",
            "Epoch 8: val_loss improved from 2.08051 to 1.96034, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - categorical_accuracy: 0.5811 - loss: 1.7658 - val_categorical_accuracy: 0.5897 - val_loss: 1.9603 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5626 - loss: 1.8303\n",
            "Epoch 9: val_loss improved from 1.96034 to 1.80554, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.5646 - loss: 1.8108 - val_categorical_accuracy: 0.6154 - val_loss: 1.8055 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6345 - loss: 1.5789\n",
            "Epoch 10: val_loss improved from 1.80554 to 1.67316, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6339 - loss: 1.5814 - val_categorical_accuracy: 0.6282 - val_loss: 1.6732 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6344 - loss: 1.4466\n",
            "Epoch 11: val_loss improved from 1.67316 to 1.59083, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6335 - loss: 1.4471 - val_categorical_accuracy: 0.6923 - val_loss: 1.5908 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6007 - loss: 1.4335\n",
            "Epoch 12: val_loss did not improve from 1.59083\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.6034 - loss: 1.4323 - val_categorical_accuracy: 0.6795 - val_loss: 1.6164 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.6290 - loss: 1.4372\n",
            "Epoch 13: val_loss did not improve from 1.59083\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.6292 - loss: 1.4377 - val_categorical_accuracy: 0.6795 - val_loss: 1.6202 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6653 - loss: 1.2258\n",
            "Epoch 14: val_loss improved from 1.59083 to 1.48213, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6644 - loss: 1.2366 - val_categorical_accuracy: 0.7051 - val_loss: 1.4821 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6790 - loss: 1.2110\n",
            "Epoch 15: val_loss improved from 1.48213 to 1.38386, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6789 - loss: 1.2127 - val_categorical_accuracy: 0.7051 - val_loss: 1.3839 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6418 - loss: 1.2988\n",
            "Epoch 16: val_loss did not improve from 1.38386\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.6424 - loss: 1.2957 - val_categorical_accuracy: 0.7051 - val_loss: 1.4827 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6625 - loss: 1.1857\n",
            "Epoch 17: val_loss did not improve from 1.38386\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.6645 - loss: 1.1797 - val_categorical_accuracy: 0.7179 - val_loss: 1.4208 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7127 - loss: 1.0520\n",
            "Epoch 18: val_loss improved from 1.38386 to 1.36144, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.7138 - loss: 1.0488 - val_categorical_accuracy: 0.7179 - val_loss: 1.3614 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7125 - loss: 0.8980\n",
            "Epoch 19: val_loss did not improve from 1.36144\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7127 - loss: 0.9047 - val_categorical_accuracy: 0.7179 - val_loss: 1.3670 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6920 - loss: 1.1739\n",
            "Epoch 20: val_loss improved from 1.36144 to 1.31595, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6931 - loss: 1.1704 - val_categorical_accuracy: 0.7308 - val_loss: 1.3159 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7133 - loss: 0.9940\n",
            "Epoch 21: val_loss improved from inf to 1.31763, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - categorical_accuracy: 0.7116 - loss: 0.9969 - val_categorical_accuracy: 0.7436 - val_loss: 1.3176 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7055 - loss: 1.0451\n",
            "Epoch 22: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7049 - loss: 1.0486 - val_categorical_accuracy: 0.7308 - val_loss: 1.3213 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7211 - loss: 0.8662\n",
            "Epoch 23: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7207 - loss: 0.8755 - val_categorical_accuracy: 0.7308 - val_loss: 1.3292 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7075 - loss: 0.9602\n",
            "Epoch 24: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7082 - loss: 0.9581 - val_categorical_accuracy: 0.7308 - val_loss: 1.3337 - learning_rate: 1.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7313 - loss: 1.2050\n",
            "Epoch 25: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7313 - loss: 1.1942 - val_categorical_accuracy: 0.7308 - val_loss: 1.3276 - learning_rate: 1.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - categorical_accuracy: 0.7213 - loss: 1.0034\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.7218 - loss: 1.0018 - val_categorical_accuracy: 0.7308 - val_loss: 1.3276 - learning_rate: 1.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m72/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7347 - loss: 1.0444\n",
            "Epoch 27: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - categorical_accuracy: 0.7343 - loss: 1.0449 - val_categorical_accuracy: 0.7308 - val_loss: 1.3277 - learning_rate: 1.0000e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m70/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7523 - loss: 0.8644\n",
            "Epoch 28: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7497 - loss: 0.8788 - val_categorical_accuracy: 0.7308 - val_loss: 1.3282 - learning_rate: 1.0000e-06\n",
            "Epoch 29/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7238 - loss: 0.9690\n",
            "Epoch 29: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7239 - loss: 0.9734 - val_categorical_accuracy: 0.7308 - val_loss: 1.3284 - learning_rate: 1.0000e-06\n",
            "Epoch 30/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.7020 - loss: 0.9319\n",
            "Epoch 30: val_loss did not improve from 1.31763\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - categorical_accuracy: 0.7021 - loss: 0.9319 - val_categorical_accuracy: 0.7308 - val_loss: 1.3286 - learning_rate: 1.0000e-06\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "{'model': 'vgg16', 'kappa': 0.7032026212873903, 'accuracy': 0.7948717948717948, 'recall': 0.7948717948717948, 'precision': 0.7925831291215907, 'f1': 0.7850912409700691, 'specificity': 0.912647374766688, 'roc': 0.8235184780761554}\n",
            "\n",
            "Executando modelo inception_resnet_v2\n",
            "Found 585 validated image filenames belonging to 4 classes.\n",
            "Found 78 validated image filenames belonging to 4 classes.\n",
            "Found 117 validated image filenames belonging to 4 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m 1/74\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:03\u001b[0m 17s/step - categorical_accuracy: 0.3750 - loss: 1.8154"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.3461 - loss: 1.5760\n",
            "Epoch 1: val_loss improved from inf to 1.15025, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 131ms/step - categorical_accuracy: 0.3474 - loss: 1.5728 - val_categorical_accuracy: 0.4615 - val_loss: 1.1502 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.5102 - loss: 1.1887\n",
            "Epoch 2: val_loss improved from 1.15025 to 0.95390, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - categorical_accuracy: 0.5112 - loss: 1.1869 - val_categorical_accuracy: 0.6667 - val_loss: 0.9539 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.5965 - loss: 1.0687\n",
            "Epoch 3: val_loss improved from 0.95390 to 0.88891, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.5967 - loss: 1.0668 - val_categorical_accuracy: 0.6154 - val_loss: 0.8889 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.6890 - loss: 0.7942\n",
            "Epoch 4: val_loss improved from 0.88891 to 0.81644, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.6877 - loss: 0.7962 - val_categorical_accuracy: 0.7051 - val_loss: 0.8164 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.7126 - loss: 0.7633\n",
            "Epoch 5: val_loss improved from 0.81644 to 0.75772, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.7121 - loss: 0.7642 - val_categorical_accuracy: 0.7051 - val_loss: 0.7577 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.6764 - loss: 0.8306\n",
            "Epoch 6: val_loss improved from 0.75772 to 0.71877, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.6768 - loss: 0.8299 - val_categorical_accuracy: 0.7308 - val_loss: 0.7188 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.7031 - loss: 0.7503\n",
            "Epoch 7: val_loss improved from 0.71877 to 0.70605, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - categorical_accuracy: 0.7032 - loss: 0.7502 - val_categorical_accuracy: 0.7308 - val_loss: 0.7061 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7324 - loss: 0.7204\n",
            "Epoch 8: val_loss improved from 0.70605 to 0.69042, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - categorical_accuracy: 0.7319 - loss: 0.7209 - val_categorical_accuracy: 0.7436 - val_loss: 0.6904 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7224 - loss: 0.7368\n",
            "Epoch 9: val_loss improved from 0.69042 to 0.66295, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.7228 - loss: 0.7356 - val_categorical_accuracy: 0.7308 - val_loss: 0.6629 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.7346 - loss: 0.6537\n",
            "Epoch 10: val_loss improved from 0.66295 to 0.65645, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.7345 - loss: 0.6542 - val_categorical_accuracy: 0.7308 - val_loss: 0.6565 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.7292 - loss: 0.6532\n",
            "Epoch 11: val_loss improved from 0.65645 to 0.60450, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - categorical_accuracy: 0.7294 - loss: 0.6528 - val_categorical_accuracy: 0.7821 - val_loss: 0.6045 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.7676 - loss: 0.6506\n",
            "Epoch 12: val_loss improved from 0.60450 to 0.60347, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.7679 - loss: 0.6494 - val_categorical_accuracy: 0.7821 - val_loss: 0.6035 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7955 - loss: 0.5926\n",
            "Epoch 13: val_loss did not improve from 0.60347\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - categorical_accuracy: 0.7949 - loss: 0.5930 - val_categorical_accuracy: 0.7949 - val_loss: 0.6101 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7489 - loss: 0.6288\n",
            "Epoch 14: val_loss improved from 0.60347 to 0.55258, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.7491 - loss: 0.6283 - val_categorical_accuracy: 0.7692 - val_loss: 0.5526 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - categorical_accuracy: 0.7905 - loss: 0.5534\n",
            "Epoch 15: val_loss did not improve from 0.55258\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - categorical_accuracy: 0.7904 - loss: 0.5536 - val_categorical_accuracy: 0.7821 - val_loss: 0.5640 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7881 - loss: 0.5799\n",
            "Epoch 16: val_loss did not improve from 0.55258\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - categorical_accuracy: 0.7884 - loss: 0.5791 - val_categorical_accuracy: 0.7436 - val_loss: 0.5616 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7456 - loss: 0.6227\n",
            "Epoch 17: val_loss did not improve from 0.55258\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - categorical_accuracy: 0.7464 - loss: 0.6208 - val_categorical_accuracy: 0.7692 - val_loss: 0.5549 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.7796 - loss: 0.5490\n",
            "Epoch 18: val_loss improved from 0.55258 to 0.55243, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.7805 - loss: 0.5484 - val_categorical_accuracy: 0.7436 - val_loss: 0.5524 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8138 - loss: 0.5133\n",
            "Epoch 19: val_loss improved from 0.55243 to 0.51566, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - categorical_accuracy: 0.8140 - loss: 0.5135 - val_categorical_accuracy: 0.7821 - val_loss: 0.5157 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8035 - loss: 0.5119\n",
            "Epoch 20: val_loss did not improve from 0.51566\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - categorical_accuracy: 0.8035 - loss: 0.5121 - val_categorical_accuracy: 0.7692 - val_loss: 0.5658 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - categorical_accuracy: 0.6244 - loss: 0.9911\n",
            "Epoch 21: val_loss improved from inf to 0.56201, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 326ms/step - categorical_accuracy: 0.6252 - loss: 0.9900 - val_categorical_accuracy: 0.8205 - val_loss: 0.5620 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - categorical_accuracy: 0.7300 - loss: 0.7204\n",
            "Epoch 22: val_loss improved from 0.56201 to 0.54096, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 233ms/step - categorical_accuracy: 0.7300 - loss: 0.7204 - val_categorical_accuracy: 0.7821 - val_loss: 0.5410 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - categorical_accuracy: 0.8160 - loss: 0.5738\n",
            "Epoch 23: val_loss improved from 0.54096 to 0.54091, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 232ms/step - categorical_accuracy: 0.8161 - loss: 0.5733 - val_categorical_accuracy: 0.7692 - val_loss: 0.5409 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - categorical_accuracy: 0.8248 - loss: 0.5196\n",
            "Epoch 24: val_loss improved from 0.54091 to 0.52509, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 234ms/step - categorical_accuracy: 0.8248 - loss: 0.5197 - val_categorical_accuracy: 0.7564 - val_loss: 0.5251 - learning_rate: 1.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - categorical_accuracy: 0.8572 - loss: 0.4580\n",
            "Epoch 25: val_loss improved from 0.52509 to 0.48449, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 234ms/step - categorical_accuracy: 0.8571 - loss: 0.4582 - val_categorical_accuracy: 0.8077 - val_loss: 0.4845 - learning_rate: 1.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - categorical_accuracy: 0.8562 - loss: 0.4184\n",
            "Epoch 26: val_loss improved from 0.48449 to 0.47275, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 230ms/step - categorical_accuracy: 0.8562 - loss: 0.4184 - val_categorical_accuracy: 0.8333 - val_loss: 0.4727 - learning_rate: 1.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - categorical_accuracy: 0.8727 - loss: 0.3784\n",
            "Epoch 27: val_loss did not improve from 0.47275\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 196ms/step - categorical_accuracy: 0.8724 - loss: 0.3789 - val_categorical_accuracy: 0.8077 - val_loss: 0.4730 - learning_rate: 1.0000e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - categorical_accuracy: 0.8886 - loss: 0.3498\n",
            "Epoch 28: val_loss improved from 0.47275 to 0.42690, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 232ms/step - categorical_accuracy: 0.8883 - loss: 0.3502 - val_categorical_accuracy: 0.8333 - val_loss: 0.4269 - learning_rate: 1.0000e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - categorical_accuracy: 0.8791 - loss: 0.3707\n",
            "Epoch 29: val_loss improved from 0.42690 to 0.42150, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 233ms/step - categorical_accuracy: 0.8791 - loss: 0.3705 - val_categorical_accuracy: 0.8462 - val_loss: 0.4215 - learning_rate: 1.0000e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - categorical_accuracy: 0.8688 - loss: 0.3464\n",
            "Epoch 30: val_loss improved from 0.42150 to 0.40169, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 233ms/step - categorical_accuracy: 0.8691 - loss: 0.3461 - val_categorical_accuracy: 0.8590 - val_loss: 0.4017 - learning_rate: 1.0000e-05\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 426ms/step\n",
            "{'model': 'inception_resnet_v2', 'kappa': 0.8418261231281198, 'accuracy': 0.8888888888888888, 'recall': 0.8888888888888888, 'precision': 0.8900666845947284, 'f1': 0.8875291375291375, 'specificity': 0.9585096167844632, 'roc': 0.9132438921565373}\n",
            "\n",
            "======================================================\n",
            "Iteração 2 de 4\n",
            "======================================================\n",
            "\n",
            "Executando modelo vgg16\n",
            "Found 585 validated image filenames belonging to 4 classes.\n",
            "Found 78 validated image filenames belonging to 4 classes.\n",
            "Found 117 validated image filenames belonging to 4 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m 1/74\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 2s/step - categorical_accuracy: 0.5000 - loss: 2.5291"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.3399 - loss: 4.4300\n",
            "Epoch 1: val_loss improved from inf to 3.57555, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - categorical_accuracy: 0.3403 - loss: 4.4051 - val_categorical_accuracy: 0.3590 - val_loss: 3.5755 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.3359 - loss: 2.9792\n",
            "Epoch 2: val_loss improved from 3.57555 to 2.69202, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.3370 - loss: 2.9848 - val_categorical_accuracy: 0.4359 - val_loss: 2.6920 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.3993 - loss: 2.7904\n",
            "Epoch 3: val_loss improved from 2.69202 to 2.43461, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.4019 - loss: 2.7760 - val_categorical_accuracy: 0.4615 - val_loss: 2.4346 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.4450 - loss: 2.5441\n",
            "Epoch 4: val_loss improved from 2.43461 to 2.17588, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.4459 - loss: 2.5374 - val_categorical_accuracy: 0.5513 - val_loss: 2.1759 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5105 - loss: 2.1319\n",
            "Epoch 5: val_loss improved from 2.17588 to 2.02981, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.5104 - loss: 2.1317 - val_categorical_accuracy: 0.5513 - val_loss: 2.0298 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.4833 - loss: 2.1328\n",
            "Epoch 6: val_loss improved from 2.02981 to 1.88219, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.4840 - loss: 2.1342 - val_categorical_accuracy: 0.6026 - val_loss: 1.8822 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5021 - loss: 2.1006\n",
            "Epoch 7: val_loss improved from 1.88219 to 1.81176, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - categorical_accuracy: 0.5042 - loss: 2.0904 - val_categorical_accuracy: 0.6026 - val_loss: 1.8118 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6155 - loss: 1.9382\n",
            "Epoch 8: val_loss improved from 1.81176 to 1.73984, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6128 - loss: 1.9333 - val_categorical_accuracy: 0.6410 - val_loss: 1.7398 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5457 - loss: 1.9189\n",
            "Epoch 9: val_loss improved from 1.73984 to 1.63823, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.5473 - loss: 1.9150 - val_categorical_accuracy: 0.6538 - val_loss: 1.6382 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6069 - loss: 1.6289\n",
            "Epoch 10: val_loss improved from 1.63823 to 1.55391, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6060 - loss: 1.6344 - val_categorical_accuracy: 0.6538 - val_loss: 1.5539 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.5969 - loss: 1.6034\n",
            "Epoch 11: val_loss improved from 1.55391 to 1.53189, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.5969 - loss: 1.6033 - val_categorical_accuracy: 0.6538 - val_loss: 1.5319 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6036 - loss: 1.3273\n",
            "Epoch 12: val_loss improved from 1.53189 to 1.46139, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6036 - loss: 1.3341 - val_categorical_accuracy: 0.6667 - val_loss: 1.4614 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6689 - loss: 1.4799\n",
            "Epoch 13: val_loss improved from 1.46139 to 1.41889, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6676 - loss: 1.4717 - val_categorical_accuracy: 0.6538 - val_loss: 1.4189 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - categorical_accuracy: 0.6803 - loss: 1.3223\n",
            "Epoch 14: val_loss improved from 1.41889 to 1.36368, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6801 - loss: 1.3223 - val_categorical_accuracy: 0.6667 - val_loss: 1.3637 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6519 - loss: 1.2469\n",
            "Epoch 15: val_loss improved from 1.36368 to 1.32236, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6522 - loss: 1.2513 - val_categorical_accuracy: 0.6667 - val_loss: 1.3224 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6046 - loss: 1.5100\n",
            "Epoch 16: val_loss improved from 1.32236 to 1.26503, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6084 - loss: 1.4952 - val_categorical_accuracy: 0.6667 - val_loss: 1.2650 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6328 - loss: 1.3631\n",
            "Epoch 17: val_loss improved from 1.26503 to 1.21070, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6326 - loss: 1.3661 - val_categorical_accuracy: 0.6923 - val_loss: 1.2107 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.6578 - loss: 1.3606\n",
            "Epoch 18: val_loss improved from 1.21070 to 1.15433, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6579 - loss: 1.3566 - val_categorical_accuracy: 0.6923 - val_loss: 1.1543 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - categorical_accuracy: 0.6690 - loss: 1.2423\n",
            "Epoch 19: val_loss improved from 1.15433 to 1.11956, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.6696 - loss: 1.2408 - val_categorical_accuracy: 0.6795 - val_loss: 1.1196 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6646 - loss: 1.1754\n",
            "Epoch 20: val_loss improved from 1.11956 to 1.11423, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6658 - loss: 1.1741 - val_categorical_accuracy: 0.6923 - val_loss: 1.1142 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6693 - loss: 1.1934\n",
            "Epoch 21: val_loss improved from inf to 1.10675, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - categorical_accuracy: 0.6722 - loss: 1.1871 - val_categorical_accuracy: 0.6923 - val_loss: 1.1068 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6479 - loss: 1.2179\n",
            "Epoch 22: val_loss improved from 1.10675 to 1.10200, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6496 - loss: 1.2129 - val_categorical_accuracy: 0.6923 - val_loss: 1.1020 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6724 - loss: 1.1764\n",
            "Epoch 23: val_loss improved from 1.10200 to 1.09593, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6729 - loss: 1.1696 - val_categorical_accuracy: 0.6923 - val_loss: 1.0959 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m69/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7334 - loss: 1.0205\n",
            "Epoch 24: val_loss improved from 1.09593 to 1.08733, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.7304 - loss: 1.0289 - val_categorical_accuracy: 0.6923 - val_loss: 1.0873 - learning_rate: 1.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.6771 - loss: 1.1638\n",
            "Epoch 25: val_loss improved from 1.08733 to 1.08435, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6775 - loss: 1.1584 - val_categorical_accuracy: 0.6923 - val_loss: 1.0843 - learning_rate: 1.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7016 - loss: 0.9796\n",
            "Epoch 26: val_loss improved from 1.08435 to 1.07473, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.7012 - loss: 0.9853 - val_categorical_accuracy: 0.6923 - val_loss: 1.0747 - learning_rate: 1.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7289 - loss: 0.9263\n",
            "Epoch 27: val_loss improved from 1.07473 to 1.07068, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.7285 - loss: 0.9271 - val_categorical_accuracy: 0.6923 - val_loss: 1.0707 - learning_rate: 1.0000e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.7085 - loss: 1.1456\n",
            "Epoch 28: val_loss improved from 1.07068 to 1.06174, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.7086 - loss: 1.1448 - val_categorical_accuracy: 0.6923 - val_loss: 1.0617 - learning_rate: 1.0000e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - categorical_accuracy: 0.7131 - loss: 1.1787\n",
            "Epoch 29: val_loss improved from 1.06174 to 1.05992, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - categorical_accuracy: 0.7126 - loss: 1.1757 - val_categorical_accuracy: 0.6923 - val_loss: 1.0599 - learning_rate: 1.0000e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m71/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.6955 - loss: 1.1312\n",
            "Epoch 30: val_loss improved from 1.05992 to 1.05445, saving model to modelo/vgg16.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - categorical_accuracy: 0.6954 - loss: 1.1299 - val_categorical_accuracy: 0.6923 - val_loss: 1.0544 - learning_rate: 1.0000e-05\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "{'model': 'vgg16', 'kappa': 0.5046046363925056, 'accuracy': 0.6581196581196581, 'recall': 0.6581196581196581, 'precision': 0.6368415821904195, 'f1': 0.6437523833524417, 'specificity': 0.8678215435954502, 'roc': 0.7135862174658505}\n",
            "\n",
            "Executando modelo inception_resnet_v2\n",
            "Found 585 validated image filenames belonging to 4 classes.\n",
            "Found 78 validated image filenames belonging to 4 classes.\n",
            "Found 117 validated image filenames belonging to 4 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m 1/74\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20:50\u001b[0m 17s/step - categorical_accuracy: 0.2500 - loss: 1.6011"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - categorical_accuracy: 0.3663 - loss: 1.3603\n",
            "Epoch 1: val_loss improved from inf to 0.83668, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 131ms/step - categorical_accuracy: 0.3697 - loss: 1.3562 - val_categorical_accuracy: 0.7692 - val_loss: 0.8367 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.6859 - loss: 0.9313\n",
            "Epoch 2: val_loss improved from 0.83668 to 0.67615, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.6856 - loss: 0.9309 - val_categorical_accuracy: 0.7692 - val_loss: 0.6761 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7088 - loss: 0.8323\n",
            "Epoch 3: val_loss improved from 0.67615 to 0.57072, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.7101 - loss: 0.8306 - val_categorical_accuracy: 0.8077 - val_loss: 0.5707 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.7768 - loss: 0.6945\n",
            "Epoch 4: val_loss improved from 0.57072 to 0.49302, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.7772 - loss: 0.6937 - val_categorical_accuracy: 0.8590 - val_loss: 0.4930 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8245 - loss: 0.5940\n",
            "Epoch 5: val_loss improved from 0.49302 to 0.43760, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.8240 - loss: 0.5948 - val_categorical_accuracy: 0.8846 - val_loss: 0.4376 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.8682 - loss: 0.5585\n",
            "Epoch 6: val_loss improved from 0.43760 to 0.39717, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.8679 - loss: 0.5584 - val_categorical_accuracy: 0.8846 - val_loss: 0.3972 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.8292 - loss: 0.5310\n",
            "Epoch 7: val_loss improved from 0.39717 to 0.36228, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - categorical_accuracy: 0.8298 - loss: 0.5301 - val_categorical_accuracy: 0.9359 - val_loss: 0.3623 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.8600 - loss: 0.4989\n",
            "Epoch 8: val_loss improved from 0.36228 to 0.33785, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - categorical_accuracy: 0.8602 - loss: 0.4984 - val_categorical_accuracy: 0.9359 - val_loss: 0.3378 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8632 - loss: 0.4764\n",
            "Epoch 9: val_loss improved from 0.33785 to 0.31446, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.8639 - loss: 0.4755 - val_categorical_accuracy: 0.9359 - val_loss: 0.3145 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - categorical_accuracy: 0.8980 - loss: 0.4110\n",
            "Epoch 10: val_loss improved from 0.31446 to 0.29826, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.8978 - loss: 0.4115 - val_categorical_accuracy: 0.9359 - val_loss: 0.2983 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8999 - loss: 0.3703\n",
            "Epoch 11: val_loss improved from 0.29826 to 0.28725, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.8994 - loss: 0.3723 - val_categorical_accuracy: 0.9359 - val_loss: 0.2872 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8935 - loss: 0.4146\n",
            "Epoch 12: val_loss improved from 0.28725 to 0.27482, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.8936 - loss: 0.4143 - val_categorical_accuracy: 0.9359 - val_loss: 0.2748 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.9053 - loss: 0.3505\n",
            "Epoch 13: val_loss improved from 0.27482 to 0.26540, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - categorical_accuracy: 0.9049 - loss: 0.3508 - val_categorical_accuracy: 0.9359 - val_loss: 0.2654 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - categorical_accuracy: 0.8673 - loss: 0.3714\n",
            "Epoch 14: val_loss improved from 0.26540 to 0.25210, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - categorical_accuracy: 0.8679 - loss: 0.3709 - val_categorical_accuracy: 0.9359 - val_loss: 0.2521 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.9008 - loss: 0.4008\n",
            "Epoch 15: val_loss improved from 0.25210 to 0.24528, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.9010 - loss: 0.4002 - val_categorical_accuracy: 0.9359 - val_loss: 0.2453 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.9156 - loss: 0.3266\n",
            "Epoch 16: val_loss improved from 0.24528 to 0.23976, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - categorical_accuracy: 0.9156 - loss: 0.3269 - val_categorical_accuracy: 0.9359 - val_loss: 0.2398 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.9276 - loss: 0.3349\n",
            "Epoch 17: val_loss improved from 0.23976 to 0.23219, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.9275 - loss: 0.3347 - val_categorical_accuracy: 0.9359 - val_loss: 0.2322 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - categorical_accuracy: 0.9129 - loss: 0.3049\n",
            "Epoch 18: val_loss improved from 0.23219 to 0.22920, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - categorical_accuracy: 0.9127 - loss: 0.3055 - val_categorical_accuracy: 0.9359 - val_loss: 0.2292 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.8875 - loss: 0.3345\n",
            "Epoch 19: val_loss improved from 0.22920 to 0.22094, saving model to modelo/inception_resnet_v2.weights.h5\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - categorical_accuracy: 0.8877 - loss: 0.3341 - val_categorical_accuracy: 0.9359 - val_loss: 0.2209 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m73/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - categorical_accuracy: 0.9377 - loss: 0.2712\n",
            "Epoch 20: val_loss did not improve from 0.22094\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - categorical_accuracy: 0.9371 - loss: 0.2719 - val_categorical_accuracy: 0.9359 - val_loss: 0.2218 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n"
          ]
        }
      ],
      "source": [
        "# k-fold\n",
        "kfold = StratifiedKFold(n_splits = N_SPLIT, shuffle = True, random_state = SEED)\n",
        "\n",
        "# Contador de iterações do k-fold\n",
        "iteracao = 1\n",
        "\n",
        "# K-fold treino e teste de cada divisão\n",
        "for train_idx, test_idx in list(kfold.split(dataset_x, dataset_y)):\n",
        "\n",
        "    print(\"\\n======================================================\")\n",
        "    print(\"Iteração {} de {}\".format(iteracao, N_SPLIT))\n",
        "    print(\"======================================================\")\n",
        "\n",
        "    train_df = dataset.iloc[train_idx]\n",
        "    test_temp = dataset.iloc[test_idx]\n",
        "\n",
        "    # dividir o teste em validação e teste\n",
        "    val_df, test_df = train_test_split(test_temp, test_size = 0.60, random_state = SEED) # 10% validação e 15% teste\n",
        "\n",
        "    for rede in range(len(models)): # Todos os modelos\n",
        "\n",
        "        print('\\nExecutando modelo {}'.format(models[rede].name))\n",
        "\n",
        "        # Modelo base\n",
        "        base_model, img_size, preprocess_input = modelo_base(rede)\n",
        "\n",
        "        # Leitura dos dados\n",
        "        train_dataset, val_dataset, test_dataset = leitura_dados(img_size)\n",
        "\n",
        "        # Construir modelo\n",
        "        model = build_model()\n",
        "\n",
        "        # Compilar Modelo\n",
        "        compile_model(base_learning_rate)\n",
        "\n",
        "        # Treinamento do modelo\n",
        "        history = treinamento_model(epocas_treinamento, 0)\n",
        "\n",
        "        # Curvas de aprendizado da precisão / perda de treinamento e validação ao usar o modelo\n",
        "        # metricas_graficos = aprendizado_treinamento()\n",
        "\n",
        "        # Ajuste fino\n",
        "        history_fine = ajuste_fino()\n",
        "\n",
        "        # Curvas de aprendizado da precisão / perda de treinamento e validação ao ajustar as últimas camadas do modelo\n",
        "        # aprendizado_ajuste_fino()\n",
        "\n",
        "        # Carrega o melhor modelo\n",
        "        model.load_weights('modelo/{}.weights.h5'.format(models[rede].name))\n",
        "\n",
        "        # Obtemos os rótulos verdadeiros\n",
        "        y_true = np.array(test_dataset.classes)\n",
        "\n",
        "        # Obtemos os rótulos previstos\n",
        "        previsoes = model.predict(test_dataset, verbose = 1)\n",
        "        y_pred = previsoes.argmax(axis=1)\n",
        "\n",
        "        # Calcula métricas Multiclasse\n",
        "        metrics_multi_class(y_true, y_pred)\n",
        "\n",
        "        # Plota matriz de confusão\n",
        "        # plot_confusion_matrix()\n",
        "\n",
        "        # Gera o relatório de classificação\n",
        "        # report = classification_report(y_true, y_pred, target_names = CATEGORIAS)\n",
        "        # print(report)\n",
        "\n",
        "        # Limpa a sessão\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "    iteracao = iteracao + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proof-machine",
      "metadata": {
        "id": "proof-machine",
        "outputId": "e006c88c-2ad7-4217-dd74-78804cb4de37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>kappa</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>recall</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>specificity</th>\n",
              "      <th>roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.583439</td>\n",
              "      <td>0.706897</td>\n",
              "      <td>0.706897</td>\n",
              "      <td>0.702102</td>\n",
              "      <td>0.703548</td>\n",
              "      <td>0.897338</td>\n",
              "      <td>0.762178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.877727</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>0.913793</td>\n",
              "      <td>0.916971</td>\n",
              "      <td>0.912457</td>\n",
              "      <td>0.970356</td>\n",
              "      <td>0.930163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.837971</td>\n",
              "      <td>0.887931</td>\n",
              "      <td>0.887931</td>\n",
              "      <td>0.895229</td>\n",
              "      <td>0.882963</td>\n",
              "      <td>0.947585</td>\n",
              "      <td>0.893466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.899266</td>\n",
              "      <td>0.892744</td>\n",
              "      <td>0.959144</td>\n",
              "      <td>0.903801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>0.851409</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.902625</td>\n",
              "      <td>0.895572</td>\n",
              "      <td>0.951747</td>\n",
              "      <td>0.909487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>0.817361</td>\n",
              "      <td>0.870690</td>\n",
              "      <td>0.870690</td>\n",
              "      <td>0.872278</td>\n",
              "      <td>0.871355</td>\n",
              "      <td>0.959452</td>\n",
              "      <td>0.898470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.839762</td>\n",
              "      <td>0.887931</td>\n",
              "      <td>0.887931</td>\n",
              "      <td>0.895675</td>\n",
              "      <td>0.880491</td>\n",
              "      <td>0.957969</td>\n",
              "      <td>0.901108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.536401</td>\n",
              "      <td>0.681034</td>\n",
              "      <td>0.681034</td>\n",
              "      <td>0.665325</td>\n",
              "      <td>0.662627</td>\n",
              "      <td>0.867409</td>\n",
              "      <td>0.730334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.938889</td>\n",
              "      <td>0.956897</td>\n",
              "      <td>0.956897</td>\n",
              "      <td>0.959172</td>\n",
              "      <td>0.957162</td>\n",
              "      <td>0.984142</td>\n",
              "      <td>0.964816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.891227</td>\n",
              "      <td>0.922414</td>\n",
              "      <td>0.922414</td>\n",
              "      <td>0.928263</td>\n",
              "      <td>0.923501</td>\n",
              "      <td>0.983071</td>\n",
              "      <td>0.941804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.902336</td>\n",
              "      <td>0.931034</td>\n",
              "      <td>0.931034</td>\n",
              "      <td>0.931199</td>\n",
              "      <td>0.931015</td>\n",
              "      <td>0.978173</td>\n",
              "      <td>0.942007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>0.914246</td>\n",
              "      <td>0.939655</td>\n",
              "      <td>0.939655</td>\n",
              "      <td>0.939786</td>\n",
              "      <td>0.938490</td>\n",
              "      <td>0.978841</td>\n",
              "      <td>0.945207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.939171</td>\n",
              "      <td>0.956897</td>\n",
              "      <td>0.956897</td>\n",
              "      <td>0.958020</td>\n",
              "      <td>0.957095</td>\n",
              "      <td>0.989708</td>\n",
              "      <td>0.964942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.696462</td>\n",
              "      <td>0.784483</td>\n",
              "      <td>0.784483</td>\n",
              "      <td>0.798097</td>\n",
              "      <td>0.787888</td>\n",
              "      <td>0.926197</td>\n",
              "      <td>0.839296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.963245</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.974330</td>\n",
              "      <td>0.974047</td>\n",
              "      <td>0.988949</td>\n",
              "      <td>0.977339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.963507</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.975862</td>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.995454</td>\n",
              "      <td>0.981808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.975507</td>\n",
              "      <td>0.982759</td>\n",
              "      <td>0.982759</td>\n",
              "      <td>0.983457</td>\n",
              "      <td>0.982431</td>\n",
              "      <td>0.993352</td>\n",
              "      <td>0.983125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>0.987759</td>\n",
              "      <td>0.991379</td>\n",
              "      <td>0.991379</td>\n",
              "      <td>0.991600</td>\n",
              "      <td>0.991312</td>\n",
              "      <td>0.995800</td>\n",
              "      <td>0.991453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>0.927197</td>\n",
              "      <td>0.948276</td>\n",
              "      <td>0.948276</td>\n",
              "      <td>0.951553</td>\n",
              "      <td>0.948834</td>\n",
              "      <td>0.987223</td>\n",
              "      <td>0.965339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.963276</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.974138</td>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.974071</td>\n",
              "      <td>0.989153</td>\n",
              "      <td>0.977813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.687432</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.771778</td>\n",
              "      <td>0.775531</td>\n",
              "      <td>0.919047</td>\n",
              "      <td>0.814492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.975285</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.982832</td>\n",
              "      <td>0.982615</td>\n",
              "      <td>0.994200</td>\n",
              "      <td>0.986907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.950607</td>\n",
              "      <td>0.965217</td>\n",
              "      <td>0.965217</td>\n",
              "      <td>0.966055</td>\n",
              "      <td>0.964975</td>\n",
              "      <td>0.988503</td>\n",
              "      <td>0.973827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.975213</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.982832</td>\n",
              "      <td>0.982533</td>\n",
              "      <td>0.994303</td>\n",
              "      <td>0.981949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.975287</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.982609</td>\n",
              "      <td>0.983343</td>\n",
              "      <td>0.982615</td>\n",
              "      <td>0.994303</td>\n",
              "      <td>0.986920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  model     kappa  accuracy    recall  precision        f1  \\\n",
              "0                 vgg16  0.583439  0.706897  0.706897   0.702102  0.703548   \n",
              "1           densenet201  0.877727  0.913793  0.913793   0.916971  0.912457   \n",
              "2          inception_v3  0.837971  0.887931  0.887931   0.895229  0.882963   \n",
              "3              xception  0.851631  0.896552  0.896552   0.899266  0.892744   \n",
              "4   inception_resnet_v2  0.851409  0.896552  0.896552   0.902625  0.895572   \n",
              "5                NASNet  0.817361  0.870690  0.870690   0.872278  0.871355   \n",
              "6           resnet152v2  0.839762  0.887931  0.887931   0.895675  0.880491   \n",
              "7                 vgg16  0.536401  0.681034  0.681034   0.665325  0.662627   \n",
              "8           densenet201  0.938889  0.956897  0.956897   0.959172  0.957162   \n",
              "9          inception_v3  0.891227  0.922414  0.922414   0.928263  0.923501   \n",
              "10             xception  0.902336  0.931034  0.931034   0.931199  0.931015   \n",
              "11  inception_resnet_v2  1.000000  1.000000  1.000000   1.000000  1.000000   \n",
              "12               NASNet  0.914246  0.939655  0.939655   0.939786  0.938490   \n",
              "13          resnet152v2  0.939171  0.956897  0.956897   0.958020  0.957095   \n",
              "14                vgg16  0.696462  0.784483  0.784483   0.798097  0.787888   \n",
              "15          densenet201  0.963245  0.974138  0.974138   0.974330  0.974047   \n",
              "16         inception_v3  0.963507  0.974138  0.974138   0.975862  0.974359   \n",
              "17             xception  0.975507  0.982759  0.982759   0.983457  0.982431   \n",
              "18  inception_resnet_v2  0.987759  0.991379  0.991379   0.991600  0.991312   \n",
              "19               NASNet  0.927197  0.948276  0.948276   0.951553  0.948834   \n",
              "20          resnet152v2  0.963276  0.974138  0.974138   0.974359  0.974071   \n",
              "21                vgg16  0.687432  0.782609  0.782609   0.771778  0.775531   \n",
              "22          densenet201  0.975285  0.982609  0.982609   0.982832  0.982615   \n",
              "23         inception_v3  0.950607  0.965217  0.965217   0.966055  0.964975   \n",
              "24             xception  0.975213  0.982609  0.982609   0.982832  0.982533   \n",
              "25  inception_resnet_v2  1.000000  1.000000  1.000000   1.000000  1.000000   \n",
              "26               NASNet  1.000000  1.000000  1.000000   1.000000  1.000000   \n",
              "27          resnet152v2  0.975287  0.982609  0.982609   0.983343  0.982615   \n",
              "\n",
              "    specificity       roc  \n",
              "0      0.897338  0.762178  \n",
              "1      0.970356  0.930163  \n",
              "2      0.947585  0.893466  \n",
              "3      0.959144  0.903801  \n",
              "4      0.951747  0.909487  \n",
              "5      0.959452  0.898470  \n",
              "6      0.957969  0.901108  \n",
              "7      0.867409  0.730334  \n",
              "8      0.984142  0.964816  \n",
              "9      0.983071  0.941804  \n",
              "10     0.978173  0.942007  \n",
              "11     1.000000  1.000000  \n",
              "12     0.978841  0.945207  \n",
              "13     0.989708  0.964942  \n",
              "14     0.926197  0.839296  \n",
              "15     0.988949  0.977339  \n",
              "16     0.995454  0.981808  \n",
              "17     0.993352  0.983125  \n",
              "18     0.995800  0.991453  \n",
              "19     0.987223  0.965339  \n",
              "20     0.989153  0.977813  \n",
              "21     0.919047  0.814492  \n",
              "22     0.994200  0.986907  \n",
              "23     0.988503  0.973827  \n",
              "24     0.994303  0.981949  \n",
              "25     1.000000  1.000000  \n",
              "26     1.000000  1.000000  \n",
              "27     0.994303  0.986920  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "understanding-chorus",
      "metadata": {
        "id": "understanding-chorus"
      },
      "outputs": [],
      "source": [
        "# Resultados de todas as iterações\n",
        "resultados.to_csv('resultados/resultados.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "subsequent-crowd",
      "metadata": {
        "id": "subsequent-crowd"
      },
      "outputs": [],
      "source": [
        "# Média dos resultados\n",
        "def resultados_mean():\n",
        "    df_mean = pd.DataFrame(columns = colunas_dataframe)\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        lista_media = []\n",
        "        lista_media.append(models[i].name)\n",
        "        lista_media = lista_media + list(resultados.loc[resultados['model'] == models[i].name].mean())\n",
        "\n",
        "        series_mean = pd.Series(lista_media, index = df_mean.columns)\n",
        "        df_mean = df_mean.append(series_mean, ignore_index=True)\n",
        "\n",
        "    return df_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usual-robertson",
      "metadata": {
        "id": "usual-robertson",
        "outputId": "7bbb970b-6dc7-44eb-b0c4-35667ddb44e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>kappa</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>recall</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>specificity</th>\n",
              "      <th>roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.625934</td>\n",
              "      <td>0.738756</td>\n",
              "      <td>0.738756</td>\n",
              "      <td>0.734326</td>\n",
              "      <td>0.732399</td>\n",
              "      <td>0.902497</td>\n",
              "      <td>0.786575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.938787</td>\n",
              "      <td>0.956859</td>\n",
              "      <td>0.956859</td>\n",
              "      <td>0.958326</td>\n",
              "      <td>0.956570</td>\n",
              "      <td>0.984412</td>\n",
              "      <td>0.964807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.910828</td>\n",
              "      <td>0.937425</td>\n",
              "      <td>0.937425</td>\n",
              "      <td>0.941352</td>\n",
              "      <td>0.936449</td>\n",
              "      <td>0.978653</td>\n",
              "      <td>0.947726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.926172</td>\n",
              "      <td>0.948238</td>\n",
              "      <td>0.948238</td>\n",
              "      <td>0.949189</td>\n",
              "      <td>0.947181</td>\n",
              "      <td>0.981243</td>\n",
              "      <td>0.952721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>0.959792</td>\n",
              "      <td>0.971983</td>\n",
              "      <td>0.971983</td>\n",
              "      <td>0.973556</td>\n",
              "      <td>0.971721</td>\n",
              "      <td>0.986887</td>\n",
              "      <td>0.975235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>0.914701</td>\n",
              "      <td>0.939655</td>\n",
              "      <td>0.939655</td>\n",
              "      <td>0.940904</td>\n",
              "      <td>0.939670</td>\n",
              "      <td>0.981379</td>\n",
              "      <td>0.952254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.929374</td>\n",
              "      <td>0.950394</td>\n",
              "      <td>0.950394</td>\n",
              "      <td>0.952849</td>\n",
              "      <td>0.948568</td>\n",
              "      <td>0.982783</td>\n",
              "      <td>0.957696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 model     kappa  accuracy    recall  precision        f1  \\\n",
              "0                vgg16  0.625934  0.738756  0.738756   0.734326  0.732399   \n",
              "1          densenet201  0.938787  0.956859  0.956859   0.958326  0.956570   \n",
              "2         inception_v3  0.910828  0.937425  0.937425   0.941352  0.936449   \n",
              "3             xception  0.926172  0.948238  0.948238   0.949189  0.947181   \n",
              "4  inception_resnet_v2  0.959792  0.971983  0.971983   0.973556  0.971721   \n",
              "5               NASNet  0.914701  0.939655  0.939655   0.940904  0.939670   \n",
              "6          resnet152v2  0.929374  0.950394  0.950394   0.952849  0.948568   \n",
              "\n",
              "   specificity       roc  \n",
              "0     0.902497  0.786575  \n",
              "1     0.984412  0.964807  \n",
              "2     0.978653  0.947726  \n",
              "3     0.981243  0.952721  \n",
              "4     0.986887  0.975235  \n",
              "5     0.981379  0.952254  \n",
              "6     0.982783  0.957696  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Média dos resultados\n",
        "media_resultados = resultados_mean()\n",
        "media_resultados.to_csv('resultados/media_resultados.csv', index=False, header=True)\n",
        "media_resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedicated-gentleman",
      "metadata": {
        "id": "dedicated-gentleman"
      },
      "outputs": [],
      "source": [
        "# Desvio Padrão dos resultados\n",
        "def resultados_std():\n",
        "    df_std = pd.DataFrame(columns = colunas_dataframe)\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        lista_std = []\n",
        "        lista_std.append(models[i].name)\n",
        "        lista_std = lista_std + list(resultados.loc[resultados['model'] == models[i].name].std())\n",
        "\n",
        "        series_std = pd.Series(lista_std, index = df_std.columns)\n",
        "        df_std = df_std.append(series_std, ignore_index=True)\n",
        "\n",
        "    return df_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "antique-highway",
      "metadata": {
        "id": "antique-highway",
        "outputId": "11eac3ff-a294-4b0c-c1d5-65da4b46ea0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>kappa</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>recall</th>\n",
              "      <th>precision</th>\n",
              "      <th>f1</th>\n",
              "      <th>specificity</th>\n",
              "      <th>roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vgg16</td>\n",
              "      <td>0.078694</td>\n",
              "      <td>0.052791</td>\n",
              "      <td>0.052791</td>\n",
              "      <td>0.061289</td>\n",
              "      <td>0.059553</td>\n",
              "      <td>0.026416</td>\n",
              "      <td>0.049387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>densenet201</td>\n",
              "      <td>0.043430</td>\n",
              "      <td>0.030639</td>\n",
              "      <td>0.030639</td>\n",
              "      <td>0.029255</td>\n",
              "      <td>0.031252</td>\n",
              "      <td>0.010231</td>\n",
              "      <td>0.024804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>inception_v3</td>\n",
              "      <td>0.057878</td>\n",
              "      <td>0.039980</td>\n",
              "      <td>0.039980</td>\n",
              "      <td>0.036968</td>\n",
              "      <td>0.041949</td>\n",
              "      <td>0.021323</td>\n",
              "      <td>0.040092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xception</td>\n",
              "      <td>0.060452</td>\n",
              "      <td>0.042192</td>\n",
              "      <td>0.042192</td>\n",
              "      <td>0.041320</td>\n",
              "      <td>0.043654</td>\n",
              "      <td>0.016482</td>\n",
              "      <td>0.037800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>inception_resnet_v2</td>\n",
              "      <td>0.072485</td>\n",
              "      <td>0.050451</td>\n",
              "      <td>0.050451</td>\n",
              "      <td>0.047453</td>\n",
              "      <td>0.050931</td>\n",
              "      <td>0.023510</td>\n",
              "      <td>0.044017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NASNet</td>\n",
              "      <td>0.075072</td>\n",
              "      <td>0.053142</td>\n",
              "      <td>0.053142</td>\n",
              "      <td>0.052652</td>\n",
              "      <td>0.052890</td>\n",
              "      <td>0.017011</td>\n",
              "      <td>0.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>resnet152v2</td>\n",
              "      <td>0.061600</td>\n",
              "      <td>0.042994</td>\n",
              "      <td>0.042994</td>\n",
              "      <td>0.039532</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.016703</td>\n",
              "      <td>0.038788</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 model     kappa  accuracy    recall  precision        f1  \\\n",
              "0                vgg16  0.078694  0.052791  0.052791   0.061289  0.059553   \n",
              "1          densenet201  0.043430  0.030639  0.030639   0.029255  0.031252   \n",
              "2         inception_v3  0.057878  0.039980  0.039980   0.036968  0.041949   \n",
              "3             xception  0.060452  0.042192  0.042192   0.041320  0.043654   \n",
              "4  inception_resnet_v2  0.072485  0.050451  0.050451   0.047453  0.050931   \n",
              "5               NASNet  0.075072  0.053142  0.053142   0.052652  0.052890   \n",
              "6          resnet152v2  0.061600  0.042994  0.042994   0.039532  0.046607   \n",
              "\n",
              "   specificity       roc  \n",
              "0     0.026416  0.049387  \n",
              "1     0.010231  0.024804  \n",
              "2     0.021323  0.040092  \n",
              "3     0.016482  0.037800  \n",
              "4     0.023510  0.044017  \n",
              "5     0.017011  0.042400  \n",
              "6     0.016703  0.038788  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Desvio padrão dos resultados\n",
        "desvio_resultados = resultados_std()\n",
        "desvio_resultados.to_csv('resultados/std_resultados.csv', index=False, header=True)\n",
        "desvio_resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3bd6b8-5e3a-42ff-8774-b802e33e88f0",
      "metadata": {
        "id": "6e3bd6b8-5e3a-42ff-8774-b802e33e88f0"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430e2be9-4394-4819-9288-02553883bb31",
      "metadata": {
        "id": "430e2be9-4394-4819-9288-02553883bb31"
      },
      "source": [
        "![title](imagens/result.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036b80b2-5460-4109-83ca-bee44edf58f4",
      "metadata": {
        "id": "036b80b2-5460-4109-83ca-bee44edf58f4"
      },
      "source": [
        "![title](imagens/curvasssss.png)\n",
        "\n",
        "![title](imagens/matriz_confusao.png)\n",
        "\n",
        "![title](imagens/gradcam.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc8b5b9-5761-47e1-a983-d0fbbc989fda",
      "metadata": {
        "id": "5dc8b5b9-5761-47e1-a983-d0fbbc989fda"
      },
      "source": [
        "# Conclusões"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}